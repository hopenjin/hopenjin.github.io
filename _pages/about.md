---
permalink: /
title: "About me"
author_profile: true
redirect_from: 
  - /about/
  - /about.html
---

Welcome to my academic website!  I am Hongpeng Jin, a Ph.D. student in the Knight Foundation School of Computing and Information Sciences (KFSCIS) at Florida International University (FIU). I began my acdamic journey in Fall 2023 under the supervision of [Dr. Yanzhao Wu](https://yanzhaowu.me/). 

My primary **research interests focus** on Large Language Models (LLMs) and LLM Agents, emphasizing their efficient inference, deployment, and practical applications. Additionally, I have a strong interest in ensemble learning and optimization strategies, aiming to enhance the scalability, efficiency, and robustness of AI systems. **My goal** is to deeply explore the underlying mechanisms and untapped potential of large language models, aiming to uncover innovative findings with significant academic value and practical industrial applications.

I hold a Master of Science in Information Technology and Management from the University of Texas at Dallas. Before pursuing my Ph.D., I gained valuable **industry experience** as a Data Scientist at Cintra US, where I developed machine learning models to optimize dynamic pricing, incident detection, and analytics-driven decision-making. I also worked at HP Inc., Samsung Electronics America, and ZTE USA, gaining diverse experience across data analysis, quality assurance, and automation testing.

<span style="color: #B31B1B;">I am actively seeking **2025 Summer Research Internship opportunities** in areas related to Large Language Models (LLMs).</span>

Updates
======
- **Nov 2024** - Submitted *"Efficient and Learning Rate Boosted Deep Ensembles"* &nbsp;for conference review. 
- **Nov 2024** - Our paper *"CE-CoLLM: Efficient and Adaptive Large Language Models Through Cloud-Edge Collaboration"* &nbsp;is now available on arXiv ([arXiv:2411.02829](https://arxiv.org/abs/2411.02829)) and under conference review.
- **Oct 2024** - Presented our paper *"Effective Diversity Optimizations for Deep Ensembles"* &nbsp;at CogMI 2024.  
- **Sep 2024** - Our work *"DA-MoE: Dynamic Expert Allocation for Mixture-of-Experts Models"* &nbsp;has been published on arXiv ([arXiv:2409.06669](https://arxiv.org/abs/2409.06669)) and under conference review.
- **Nov 2023** - Presented our paper *"Rethinking Learning Rate Tuning in Large Language Models"* &nbsp;in CogMI 2023.  



Publications & Preprints
======
* CE-CoLLM: Efficient and Adaptive Large Language Models Through Cloud-Edge Collaboration<br>
  Hongpeng Jin, Yanzhao Wu.<br>
  arXiv preprint arXiv:2411.02829

* DA-MoE: Dynamic Expert Allocation for Mixture-of-Experts Models<br>
  Maryam Akhavan Aghdam, Hongpeng Jin, Yanzhao Wu.<br>
  arXiv preprint arXiv:2409.06669

* Effective Diversity Optimizations for Deep Ensembles<br>
  Hongpeng Jin, Maryam Akhavan Aghdam, Sai Nath Chowdary Medikonduru, Wenqi Wei, Xuyu Wang, Wenbin Zhang, Yanzhao Wu.<br>
  2024 IEEE International Conference on Cognitive Machine Intelligence (CogMI 2024)

* Rethinking Learning Rate Tuning in the Era of Large Language Models<br>
  Hongpeng Jin, Wenqi Wei, Xuyu Wang, Wenbin Zhang, and Yanzhao Wu.<br>
  2023 IEEE International Conference on Cognitive Machine Intelligence (CogMI 2023)


Research Experience
======

* **Efficient Deployment and Inference of Large Language Models**<br>
  Research on optimizing the inference effectiveness, efficiency, and scalability of Large Language Models (LLMs) through strategies such as cloud-edge collaboration, distributed AI, and ML efficiency techniques (e.g., early exit).<br>
  **Supervisor**: Dr. Yanzhao Wu<br>
  ontributions:<br>
  [1] **CE-CoLLM**: A LLM development method to optimize the inference efficiency and accuracy of LLMs on edge devices through cloud-edge collaboration and ML efficiency techniques, addressing diverse requirements such as inference accuracy, low latency, resource constraints, and privacy preservation.
  [2] **DA-MoE**: A dynamic expert allocation mechanism for Mixture-of-Experts (MoE) models that leverages attention-based token importance in Transformer architectures to dynamically adjust the number of experts per token, enhancing efficiency and predictive performance.<br><br>


* **Ensemble Learning for Model Performance and Robustness**<br>
  Research on improving key aspects of Ensemble Learning, including diversity measurement, ensemble selection strategies, voting mechanisms, training methodologies, overall ensemble performance, and applications.<br>
  **Supervisor**: Dr. Yanzhao Wu<br>
  ontributions:<br>
  1. **LREnsemble**: An ensemble construction framework, effectively utilizing diverse models, generated through learning rate (LR) tuning, to construct efficient and high-quality ensembles, avoiding the waste of sub-optimal trained models by leveraging their diversity for ensemble learning.
  2. **Synergistic Diversity Metric (SQ)**: An ensemble diversity metric, significantly improving ensemble accuracy and robustness to out-of-distribution samples by optimizing diversity among member models.<br><br>


* **Hyperparameter Optimization Strategy for Model Training**<br>
  Research on developing advanced hyperparameter optimization strategies to enhance the training efficiency, performance, and robustness of deep neural networks (DNNs) and large language models (LLMs).<br>
  **Supervisor**: Dr. Yanzhao Wu<br>
  ontributions:<br>
  1. **LRBench++**: A dynamic learning rate tuning framework, improving DNNs and LLMs training efficiency and achieving a balance between model accuracy and training cost.


Work Experience
======
You can also find my work experience on [my LinkedIn profile](https://www.linkedin.com/in/hongpeng-jin/).

* **Cintra US** (Austin, TX) \| Data Scientist \| May 2022 – Aug. 2023 
  * Built machine learning models to improve the work efficiency of our business or operation team, including, improving our dynamic pricing model, creating incident detection model, creating analytics AI model for auto-analysis reports, etc.
  * Conducted statistical analyses (AB tests) to quantify driver behaviors and preferences and measure the impact of various external interventions

* **HP Inc.** (Vancouver, WA) \| Marketing Survey Data Analyst \| Apr. 2020 – May 2022 
  * Modeled large-scale email survey data to quantify the margin effect of each customer journey experience. 
  * Developed a response priority algorithm for customer reviews using Supervised LDA topic modeling and statistical learning.
  * Assisted UX teams with power analysis, A/B testing, and general linear regression methods to optimize email survey titles and UI.

* **Samsung Electronics America** (Plano, TX) \| QA Engineer \| Mar. 2019 – Mar. 2020
  * Validated functions related to communication networks of Android devices across different wireless networks (GSM, WCDMA, 4G, and 5G) through software, field, and automation testing; analyzed emerging issues based on device logs and testing data to identify root causes.

* **ZTE USA Inc.** (Richardson, TX) \| Software Test Engineer \| Apr. 2018 – Mar. 2019 
  * Developed and implemented the "AIO" automation testing project, transitioning from manual to automated testing to enhance efficiency and quality.